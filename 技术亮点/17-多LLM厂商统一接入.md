# 17 - å¤šLLMå‚å•†ç»Ÿä¸€æ¥å…¥ ğŸ¤–

> **éš¾åº¦**: â­â­â­â­ | **ä»·å€¼**: ğŸ”¥ğŸ”¥ğŸ”¥ğŸ”¥ | **åˆ›æ–°åº¦**: â­â­â­â­

---

## ğŸ“Œ æŠ€æœ¯æ¦‚è¿°

### æ ¸å¿ƒèƒ½åŠ›

æˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒæ¥å…¥**6å¤§ä¸»æµLLMå‚å•†**ï¼Œé€šè¿‡ç»Ÿä¸€çš„æ¥å£è®¾è®¡ï¼Œå®ç°ï¼š

- ğŸ”€ **çµæ´»åˆ‡æ¢**: ä¸€é”®åˆ‡æ¢ä¸åŒLLMï¼Œæ— éœ€ä¿®æ”¹ä¸šåŠ¡ä»£ç 
- ğŸ›¡ï¸ **å‚å•†ç‹¬ç«‹**: ä¸è¢«å•ä¸€å‚å•†é”å®š
- ğŸ’° **æˆæœ¬ä¼˜åŒ–**: æ ¹æ®åœºæ™¯é€‰æ‹©æœ€ä¼˜æ€§ä»·æ¯”çš„æ¨¡å‹
- ğŸš€ **é«˜å¯ç”¨**: æŸä¸ªå‚å•†æ•…éšœæ—¶ï¼Œå¯å¿«é€Ÿåˆ‡æ¢åˆ°å¤‡ç”¨

### æ”¯æŒçš„LLMå‚å•†

| å‚å•† | æ¨¡å‹ | ç‰¹ç‚¹ | æ¨èåœºæ™¯ |
|-----|------|------|---------|
| **æ™ºè°±AI** | GLM-4-Flash | âœ… é»˜è®¤/æ€§ä»·æ¯”é«˜ | é€šç”¨å¯¹è¯ |
| **OpenAI** | GPT-3.5/4 | âœ… èƒ½åŠ›æœ€å¼º | å¤æ‚æ¨ç† |
| **é˜¿é‡Œäº‘** | é€šä¹‰åƒé—® | ä¸­æ–‡ä¼˜åŒ– | ä¸­æ–‡åœºæ™¯ |
| **ç™¾åº¦** | æ–‡å¿ƒä¸€è¨€ | æœ¬åœŸåŒ– | ä¸­æ–‡é—®ç­” |
| **è…¾è®¯** | æ··å…ƒ | ç¨³å®šå¯é  | ä¼ä¸šåº”ç”¨ |
| **å­—èŠ‚** | è±†åŒ… | æˆæœ¬ä½ | ç®€å•é—®ç­” |

---

## ğŸ” æŠ€æœ¯åŸç†

### ç»Ÿä¸€æ¥å£è®¾è®¡

```python
class LLMClient:
    """LLMå®¢æˆ·ç«¯åŸºç±»"""

    async def chat(
        self,
        message: str,
        system_prompt: str = None,
        history: list = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> dict:
        """
        ç»Ÿä¸€çš„å¯¹è¯æ¥å£

        Args:
            message: ç”¨æˆ·æ¶ˆæ¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            history: å†å²å¯¹è¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°

        Returns:
            {
                "reply": "AIå›å¤",
                "model": "æ¨¡å‹åç§°",
                "usage": {...}
            }
        """
        raise NotImplementedError
```

### å·¥å‚æ¨¡å¼å®ç°

```python
class LLMProvider(str, Enum):
    """LLMæœåŠ¡æä¾›å•†æšä¸¾"""
    ZHIPU = "zhipu"          # æ™ºè°±AI
    OPENAI = "openai"        # OpenAI
    DASHSCOPE = "dashscope"  # é˜¿é‡Œäº‘
    QIANFAN = "qianfan"      # ç™¾åº¦
    HUNYUAN = "hunyuan"      # è…¾è®¯
    DOUBAO = "doubao"        # å­—èŠ‚

def create_llm_client(
    provider: str = "zhipu",
    api_key: str = "",
    model: str = "",
    base_url: str = ""
) -> LLMClient:
    """LLMå®¢æˆ·ç«¯å·¥å‚å‡½æ•°"""
    provider_map = {
        "zhipu": LLMProvider.ZHIPU,
        "openai": LLMProvider.OPENAI,
        "dashscope": LLMProvider.DASHSCOPE,
        "qianfan": LLMProvider.QIANFAN,
        "hunyuan": LLMProvider.HUNYUAN,
        "doubao": LLMProvider.DOUBAO
    }

    provider_enum = provider_map.get(provider.lower(), LLMProvider.ZHIPU)

    return LLMClient(
        provider=provider_enum,
        api_key=api_key,
        model=model,
        base_url=base_url
    )
```

---

## ğŸ’¡ é¡¹ç›®åº”ç”¨

### ä»£ç ä½ç½®

**ä¸»å®ç°æ–‡ä»¶**: [backend/app/integrations/llm_client.py](../../backend/app/integrations/llm_client.py)

### 1. é…ç½®å„å‚å•†å‚æ•°

```python
class LLMClient:
    def __init__(self, provider: LLMProvider, api_key: str, model: str, base_url: str):
        self.provider = provider
        self.api_key = api_key
        self.model = model
        self.base_url = base_url

        # æ ¹æ®ä¸åŒå‚å•†é…ç½®å‚æ•°
        self._configure_provider()

    def _configure_provider(self):
        """é…ç½®å„å‚å•†å‚æ•°"""
        if self.provider == LLMProvider.ZHIPU:
            # æ™ºè°±AIé…ç½®
            self.base_url = self.base_url or "https://open.bigmodel.cn/api/paas/v4"
            self.model = self.model or "glm-4-flash"
            self.timeout = 60.0

        elif self.provider == LLMProvider.OPENAI:
            # OpenAIé…ç½®
            self.base_url = self.base_url or "https://api.openai.com/v1"
            self.model = self.model or "gpt-3.5-turbo"
            self.timeout = 60.0

        elif self.provider == LLMProvider.DASHSCOPE:
            # é˜¿é‡Œäº‘é…ç½®
            self.base_url = self.base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
            self.model = self.model or "qwen-turbo"
            self.timeout = 60.0

        # ... å…¶ä»–å‚å•†é…ç½®
```

### 2. ç»Ÿä¸€è¯·æ±‚å¤„ç†

```python
async def chat(self, message: str, system_prompt: str = None, history: list = None):
    """å‘é€å¯¹è¯è¯·æ±‚"""

    # æ„å»ºæ¶ˆæ¯åˆ—è¡¨
    messages = []

    # æ·»åŠ ç³»ç»Ÿæç¤ºè¯
    if system_prompt:
        messages.append({"role": "system", "content": system_prompt})
    else:
        messages.append({
            "role": "system",
            "content": self._get_default_system_prompt()
        })

    # æ·»åŠ å†å²è®°å½•
    if history:
        messages.extend(history)

    # æ·»åŠ å½“å‰æ¶ˆæ¯
    messages.append({"role": "user", "content": message})

    # è°ƒç”¨å‚å•†API
    try:
        response = await self._send_request(messages)
        return response
    except Exception as e:
        return {
            "reply": f"æŠ±æ­‰ï¼Œæˆ‘é‡åˆ°äº†ä¸€äº›é—®é¢˜ï¼š{str(e)}",
            "error": str(e)
        }
```

### 3. å‚å•†ç‰¹å®šå¤„ç†

```python
async def _send_request(self, messages: list) -> dict:
    """å‘é€APIè¯·æ±‚"""
    headers = {"Content-Type": "application/json"}

    # æ ¹æ®ä¸åŒå‚å•†è®¾ç½®Authorization
    if self.provider == LLMProvider.ZHIPU:
        headers["Authorization"] = f"Bearer {self.api_key}"
    elif self.provider == LLMProvider.OPENAI:
        headers["Authorization"] = f"Bearer {self.api_key}"
    elif self.provider == LLMProvider.DASHSCOPE:
        headers["Authorization"] = f"Bearer {self.api_key}"
    # ... å…¶ä»–å‚å•†

    # è¯·æ±‚ä½“
    payload = {
        "model": self.model,
        "messages": messages,
        "temperature": 0.7,
        "max_tokens": 2000
    }

    # å‘é€HTTPè¯·æ±‚
    async with httpx.AsyncClient(timeout=self.timeout) as client:
        url = f"{self.base_url}/chat/completions"
        response = await client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        data = response.json()

    # è§£æå“åº”
    return self._parse_response(data)
```

### 4. å“åº”ç»Ÿä¸€è§£æ

```python
def _parse_response(self, data: dict) -> dict:
    """è§£æAPIå“åº”"""
    try:
        # OpenAIå…¼å®¹æ ¼å¼ï¼ˆæ™ºè°±ã€é˜¿é‡Œã€å­—èŠ‚ï¼‰
        reply = data["choices"][0]["message"]["content"]
        usage = data.get("usage", {})

        return {
            "reply": reply,
            "model": data.get("model", self.model),
            "usage": {
                "prompt_tokens": usage.get("prompt_tokens", 0),
                "completion_tokens": usage.get("completion_tokens", 0),
                "total_tokens": usage.get("total_tokens", 0)
            }
        }
    except (KeyError, IndexError):
        # ç™¾åº¦åƒå¸†æ ¼å¼
        if "result" in data:
            return {
                "reply": data["result"],
                "model": self.model,
                "usage": {}
            }
        else:
            raise ValueError(f"æ— æ³•è§£æAPIå“åº”")
```

### 5. ç¯å¢ƒé…ç½®

```bash
# backend/.env

# é»˜è®¤ä½¿ç”¨æ™ºè°±AI
LLM_PROVIDER=zhipu
LLM_API_KEY=your_zhipu_api_key
LLM_MODEL=glm-4-flash

# å¦‚æœè¦åˆ‡æ¢åˆ°OpenAI
# LLM_PROVIDER=openai
# LLM_API_KEY=your_openai_api_key
# LLM_MODEL=gpt-3.5-turbo
```

### 6. æœåŠ¡å±‚é›†æˆ

```python
# backend/app/services/real_service.py

class RealService(DataServiceBase):
    """çœŸå®æœåŠ¡ - ä½¿ç”¨LLM"""

    def __init__(self):
        # åˆ›å»ºLLMå®¢æˆ·ç«¯
        self.llm_client = create_llm_client(
            provider=settings.LLM_PROVIDER,
            api_key=settings.LLM_API_KEY,
            model=settings.LLM_MODEL
        )

    async def chat(self, message: str, session_id: str, context: list):
        """æ™ºèƒ½å¯¹è¯"""

        # è°ƒç”¨LLM
        response = await self.llm_client.chat(
            message=message,
            history=context,
            temperature=0.7
        )

        # ç”Ÿæˆå»ºè®®é—®é¢˜
        suggestions = self._generate_suggestions(message, response["reply"])

        return {
            "reply": response["reply"],
            "suggestions": suggestions,
            "model": response["model"],
            "usage": response.get("usage", {})
        }
```

---

## ğŸ¯ æ¼”ç¤ºè¯æœ¯

### å¼€åœºä»‹ç»ï¼ˆ30ç§’ï¼‰

```
"æˆ‘ä»¬çš„ç³»ç»Ÿæ”¯æŒæ¥å…¥å¤šå®¶ä¸»æµLLMå‚å•†ã€‚

åŒ…æ‹¬æ™ºè°±AIã€OpenAIã€é˜¿é‡Œäº‘é€šä¹‰åƒé—®ã€ç™¾åº¦æ–‡å¿ƒä¸€è¨€ã€è…¾è®¯æ··å…ƒã€å­—èŠ‚è±†åŒ…ç­‰ã€‚

é€šè¿‡ç»Ÿä¸€çš„æ¥å£è®¾è®¡ï¼Œæˆ‘ä»¬å¯ä»¥çµæ´»åˆ‡æ¢ä¸åŒçš„æ¨¡å‹ã€‚"
```

### æŠ€æœ¯è®²è§£ï¼ˆ1åˆ†é’Ÿï¼‰

```
"è¿™ç§è®¾è®¡çš„ä¼˜åŠ¿åœ¨äºï¼š

â‘  å‚å•†ç‹¬ç«‹ï¼šä¸è¢«å•ä¸€å‚å•†é”å®š
   å¦‚æœæŸä¸ªå‚å•†æ¶¨ä»·æˆ–æœåŠ¡ä¸ç¨³å®šï¼Œå¯ä»¥å¿«é€Ÿåˆ‡æ¢

â‘¡ æˆæœ¬ä¼˜åŒ–ï¼šæ ¹æ®åœºæ™¯é€‰æ‹©æœ€ä¼˜æ¨¡å‹
   ç®€å•é—®ç­”ç”¨ä¾¿å®œçš„æ¨¡å‹ï¼Œå¤æ‚æ¨ç†ç”¨å¼ºå¤§çš„æ¨¡å‹

â‘¢ é«˜å¯ç”¨æ€§ï¼šæ”¯æŒæ•…éšœè½¬ç§»
   ä¸»å‚å•†æ•…éšœæ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨å‚å•†

â‘£ ç»Ÿä¸€æ¥å£ï¼šä¸šåŠ¡ä»£ç æ— éœ€ä¿®æ”¹
   åˆ‡æ¢LLMåªéœ€ä¿®æ”¹é…ç½®æ–‡ä»¶"
```

### ä»£ç å±•ç¤ºï¼ˆ1åˆ†é’Ÿï¼‰

```
ï¼ˆå±•ç¤ºä»£ç ï¼‰

"è¯·çœ‹è¿™æ®µä»£ç ï¼Œæˆ‘ä»¬å®šä¹‰äº†ç»Ÿä¸€çš„LLMClientæ¥å£ï¼š

```python
class LLMClient:
    async def chat(self, message: str, ...):
        # ç»Ÿä¸€çš„å¯¹è¯æ¥å£
        response = await self._send_request(messages)
        return self._parse_response(data)
```

ç„¶åé€šè¿‡å·¥å‚æ¨¡å¼åˆ›å»ºå…·ä½“å‚å•†çš„å®¢æˆ·ç«¯ï¼š

```python
client = create_llm_client(
    provider="zhipu",  # æˆ– "openai", "dashscope"ç­‰
    api_key="***",
    model="glm-4-flash"
)
```

ä¸šåŠ¡ä»£ç åªéœ€è°ƒç”¨ç»Ÿä¸€çš„chatæ–¹æ³•ï¼Œä¸å…³å¿ƒå…·ä½“æ˜¯å“ªä¸ªå‚å•†ã€‚"
```

### æ€»ç»“å¼ºè°ƒï¼ˆ30ç§’ï¼‰

```
"è¿™ç§å¤šå‚å•†æ”¯æŒçš„æ¶æ„è®¾è®¡ï¼š

â‘  æé«˜äº†ç³»ç»Ÿçš„çµæ´»æ€§å’Œå¯æ‰©å±•æ€§
â‘¡ é™ä½äº†å‚å•†ä¾èµ–é£é™©
â‘¢ ä¼˜åŒ–äº†æˆæœ¬å’Œæ€§èƒ½
â‘£ ç¬¦åˆå¾®æœåŠ¡æ¶æ„çš„æœ€ä½³å®è·µ

ä½“ç°äº†æˆ‘ä»¬å›¢é˜Ÿçš„æŠ€æœ¯å‰ç»æ€§ï¼"
```

---

## ğŸ“Š å¯¹æ¯”ä¼˜åŠ¿

### ä¸å•ä¸€å‚å•†å¯¹æ¯”

| ç»´åº¦ | å•ä¸€å‚å•† | å¤šå‚å•†æ”¯æŒ |
|-----|---------|----------|
| **å‚å•†é”å®š** | âŒ æ˜¯ | âœ… å¦ |
| **æ•…éšœæ¢å¤** | âŒ å›°éš¾ | âœ… å¿«é€Ÿåˆ‡æ¢ |
| **æˆæœ¬ä¼˜åŒ–** | âŒ æ— é€‰æ‹© | âœ… çµæ´»é€‰æ‹© |
| **æŠ€æœ¯æ¼”è¿›** | âš ï¸ å—é™ | âœ… éšæ—¶è·Ÿè¿› |

### ä¸ç«å“å¯¹æ¯”

| ç‰¹æ€§ | æœ¬é¡¹ç›® | å…¶ä»–å‚èµ›ä½œå“ |
|:----|:-----:|:----------:|
| LLMå‚å•†æ”¯æŒ | 6å®¶ | 1-2å®¶ |
| åˆ‡æ¢çµæ´»æ€§ | â­â­â­â­â­ | â­â­ |
| æ¶æ„è®¾è®¡ | â­â­â­â­â­ | â­â­â­ |

---

## ğŸš€ åº”ç”¨åœºæ™¯

### 1. æˆæœ¬ä¼˜åŒ–åœºæ™¯

```python
# ç®€å•é—®ç­”ç”¨ä¾¿å®œæ¨¡å‹
if is_simple_query(message):
    client = create_llm_client(provider="doubao")  # å­—èŠ‚è±†åŒ…ï¼Œæˆæœ¬ä½

# å¤æ‚æ¨ç†ç”¨å¼ºå¤§æ¨¡å‹
else:
    client = create_llm_client(provider="openai")  # GPT-4ï¼Œèƒ½åŠ›å¼º
```

### 2. æ•…éšœè½¬ç§»åœºæ™¯

```python
async def chat_with_fallback(message: str):
    """å¸¦æ•…éšœè½¬ç§»çš„å¯¹è¯"""
    try:
        # ä¼˜å…ˆä½¿ç”¨æ™ºè°±AI
        return await zhipu_client.chat(message)
    except Exception as e:
        logger.warning(f"æ™ºè°±AIå¤±è´¥: {e}, åˆ‡æ¢åˆ°OpenAI")
        # æ•…éšœæ—¶åˆ‡æ¢åˆ°OpenAI
        return await openai_client.chat(message)
```

### 3. A/Bæµ‹è¯•åœºæ™¯

```python
# å¯¹æ¯”ä¸åŒå‚å•†çš„æ•ˆæœ
results = {}
for provider in ["zhipu", "openai", "dashscope"]:
    client = create_llm_client(provider=provider)
    response = await client.chat(test_message)
    results[provider] = evaluate_response(response)

# é€‰æ‹©æ•ˆæœæœ€å¥½çš„å‚å•†
best_provider = max(results, key=results.get)
```

---

## ğŸ“ˆ æŠ€æœ¯æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               ä¸šåŠ¡æœåŠ¡å±‚                         â”‚
â”‚  (real_service.py, mock_service.py)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ è°ƒç”¨
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            LLMç»Ÿä¸€æ¥å£å±‚                        â”‚
â”‚       (create_llm_client)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ å·¥å‚æ¨¡å¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LLMClientåŸºç±»                      â”‚
â”‚         - chat()                                â”‚
â”‚         - _send_request()                      â”‚
â”‚         - _parse_response()                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ ç»§æ‰¿
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å„å‚å•†å®ç° (é€šè¿‡é…ç½®è‡ªåŠ¨é€‚é…)              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚ æ™ºè°±AI  â”‚ OpenAI  â”‚ é˜¿é‡Œäº‘  â”‚ ç™¾åº¦    â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“ HTTPè¯·æ±‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              å„å‚å•†APIç«¯ç‚¹                      â”‚
â”‚  open.bigmodel.cn/api/paas/v4                  â”‚
â”‚  api.openai.com/v1                             â”‚
â”‚  dashscope.aliyuncs.com/compatible-mode/v1      â”‚
â”‚  ...                                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ† æ€»ç»“

### æ ¸å¿ƒä»·å€¼

1. **çµæ´»æ€§**: â­â­â­â­â­
   - æ”¯æŒ6å¤§ä¸»æµå‚å•†ï¼Œå¯éšæ—¶åˆ‡æ¢

2. **å¯é æ€§**: â­â­â­â­â­
   - æ•…éšœè½¬ç§»æœºåˆ¶ï¼Œç¡®ä¿æœåŠ¡å¯ç”¨

3. **ç»æµæ€§**: â­â­â­â­â­
   - æ ¹æ®åœºæ™¯é€‰æ‹©æœ€ä¼˜æˆæœ¬æ–¹æ¡ˆ

4. **å‰ç»æ€§**: â­â­â­â­â­
   - ç¬¦åˆå¾®æœåŠ¡å’Œäº‘åŸç”Ÿæœ€ä½³å®è·µ

### æ¼”ç¤ºå»ºè®®

- **æœ€ä½³å±•ç¤ºä½ç½®**: åç«¯æ¶æ„è®²è§£
- **æ¼”ç¤ºæ—¶é•¿**: 1-2åˆ†é’Ÿ
- **å…³é”®è¯æœ¯**: "æ”¯æŒ6å¤§ä¸»æµLLMå‚å•†"
- **è§†è§‰å†²å‡»**: æ¶æ„å›¾ + å‚å•†Logoå±•ç¤º

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2026-01-02
**ä½œè€…**: å±±è¥¿æ–‡æ—…æ™ºèƒ½ä½“å›¢é˜Ÿ
